{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader,Dataset\nimport torch.nn as nn\nimport re\nimport matplotlib.pyplot as plt\nimport spacy\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom tqdm.notebook import tqdm\nspacy_eng = spacy.load(\"en_core_web_sm\")\nnltk.download('omw-1.4')\nnltk.download('stopwords')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-29T14:34:49.316453Z","iopub.execute_input":"2023-06-29T14:34:49.317554Z","iopub.status.idle":"2023-06-29T14:35:07.358187Z","shell.execute_reply.started":"2023-06-29T14:34:49.317507Z","shell.execute_reply":"2023-06-29T14:35:07.357205Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"# Loading data","metadata":{}},{"cell_type":"code","source":"pos_train = '/kaggle/input/aclimdb/aclImdb/train/pos'\npos_df = pd.DataFrame(columns=['text', 'score'])\nfor idx, file in enumerate(os.listdir(pos_train)):\n    file_path = os.path.join(pos_train, file)\n    with open(file_path, 'r', errors=\"ignore\") as r:\n        text = r.read()\n        pos_df.loc[idx, 'text'] = text\n        pos_df.loc[idx, 'score'] = int(file_path[-6:-4]) if file_path[-5] == '0' else int(file_path[-5])","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:35:07.360407Z","iopub.execute_input":"2023-06-29T14:35:07.361475Z","iopub.status.idle":"2023-06-29T14:36:28.268260Z","shell.execute_reply.started":"2023-06-29T14:35:07.361441Z","shell.execute_reply":"2023-06-29T14:36:28.267182Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"neg_train = '/kaggle/input/aclimdb/aclImdb/train/neg'\nneg_df = pd.DataFrame(columns=['text', 'score'])\nfor idx, file in enumerate(os.listdir(neg_train)):\n    file_path = os.path.join(neg_train, file)\n    with open(file_path, 'r', errors=\"ignore\") as r:\n        text = r.read()\n        neg_df.loc[idx, 'text'] = text\n        neg_df.loc[idx, 'score'] = int(file_path[-6:-4]) if file_path[-5] == '0' else int(file_path[-5])","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:36:28.269638Z","iopub.execute_input":"2023-06-29T14:36:28.270024Z","iopub.status.idle":"2023-06-29T14:37:49.378543Z","shell.execute_reply.started":"2023-06-29T14:36:28.269983Z","shell.execute_reply":"2023-06-29T14:37:49.377325Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([pos_df, neg_df])\ndf = df.sample(frac=1).reset_index(drop=True)\ndf.head()\nlen(df)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:37:49.382429Z","iopub.execute_input":"2023-06-29T14:37:49.383178Z","iopub.status.idle":"2023-06-29T14:37:49.397185Z","shell.execute_reply.started":"2023-06-29T14:37:49.383150Z","shell.execute_reply":"2023-06-29T14:37:49.395972Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"25000"},"metadata":{}}]},{"cell_type":"code","source":"pos_test = '/kaggle/input/aclimdb/aclImdb/test/pos'\npos_df = pd.DataFrame(columns=['text', 'score'])\nfor idx, file in enumerate(os.listdir(pos_test)):\n    file_path = os.path.join(pos_test, file)\n    with open(file_path, 'r', errors=\"ignore\") as r:\n        text = r.read()\n        pos_df.loc[idx, 'text'] = text\n        pos_df.loc[idx, 'score'] = int(file_path[-6:-4]) if file_path[-5] == '0' else int(file_path[-5])","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:37:49.398783Z","iopub.execute_input":"2023-06-29T14:37:49.399511Z","iopub.status.idle":"2023-06-29T14:39:17.044206Z","shell.execute_reply.started":"2023-06-29T14:37:49.399479Z","shell.execute_reply":"2023-06-29T14:39:17.043256Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"neg_test = '/kaggle/input/aclimdb/aclImdb/test/neg'\nneg_df = pd.DataFrame(columns=['text', 'score'])\nfor idx, file in enumerate(os.listdir(neg_test)):\n    file_path = os.path.join(neg_test, file)\n    with open(file_path, 'r', errors=\"ignore\") as r:\n        text = r.read()\n        neg_df.loc[idx, 'text'] = text\n        neg_df.loc[idx, 'score'] = int(file_path[-6:-4]) if file_path[-5] == '0' else int(file_path[-5])","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:39:17.045542Z","iopub.execute_input":"2023-06-29T14:39:17.045881Z","iopub.status.idle":"2023-06-29T14:40:43.673373Z","shell.execute_reply.started":"2023-06-29T14:39:17.045852Z","shell.execute_reply":"2023-06-29T14:40:43.672395Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df_test = pd.concat([pos_df, neg_df])\ndf_test = df_test.sample(frac=1).reset_index(drop=True)\ndf_test.head()\nlen(df_test)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:40:43.674806Z","iopub.execute_input":"2023-06-29T14:40:43.675158Z","iopub.status.idle":"2023-06-29T14:40:43.688064Z","shell.execute_reply.started":"2023-06-29T14:40:43.675127Z","shell.execute_reply":"2023-06-29T14:40:43.687006Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"25000"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data preprocessing","metadata":{}},{"cell_type":"code","source":"def lemmatize_text(text):\n    lemm=WordNetLemmatizer()\n    text = text.split()\n    text = list(map(lemm.lemmatize, text))\n    return ' '.join(text)\n\ndef remove_stopwords(text):\n    stop_words = stopwords.words(\"english\")\n    no_stop = []\n    for word in text.split(' '):\n        if word not in stop_words:\n            no_stop.append(word)\n    return \" \".join(no_stop)\n\ndef remove_punctuation_func(text):\n    return re.sub(r'[^a-zA-Z0-9]', ' ', text)\n\ndef text_preporation(text):\n    text = text.lower()\n    text = remove_stopwords(text)\n    text = remove_punctuation_func(text)\n    text = lemmatize_text(text)\n    text = re.sub(r'\\bbr\\b', '', text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:40:43.689396Z","iopub.execute_input":"2023-06-29T14:40:43.689910Z","iopub.status.idle":"2023-06-29T14:40:43.698797Z","shell.execute_reply.started":"2023-06-29T14:40:43.689880Z","shell.execute_reply":"2023-06-29T14:40:43.697746Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:40:43.700470Z","iopub.execute_input":"2023-06-29T14:40:43.700895Z","iopub.status.idle":"2023-06-29T14:40:44.988627Z","shell.execute_reply.started":"2023-06-29T14:40:43.700864Z","shell.execute_reply":"2023-06-29T14:40:44.987527Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"df['new_text'] = df['text'].map(text_preporation)\ndf['score'] = df['score'] - 1\ndf = df.drop(['text'], axis=1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:40:44.994207Z","iopub.execute_input":"2023-06-29T14:40:44.994513Z","iopub.status.idle":"2023-06-29T14:41:27.447814Z","shell.execute_reply.started":"2023-06-29T14:40:44.994486Z","shell.execute_reply":"2023-06-29T14:41:27.446422Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"  score                                           new_text\n0     7  paul lukas played russian intellectual making ...\n1     6  film like the texas chainsaw massacre suspiria...\n2     0  ever hear three word uttered you joe baker afr...\n3     1  believe lame pointless wa basically nothing la...\n4     7  minimal budget running time eight minute great...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>score</th>\n      <th>new_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>paul lukas played russian intellectual making ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>film like the texas chainsaw massacre suspiria...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>ever hear three word uttered you joe baker afr...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>believe lame pointless wa basically nothing la...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>minimal budget running time eight minute great...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.score.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:41:27.449397Z","iopub.execute_input":"2023-06-29T14:41:27.450074Z","iopub.status.idle":"2023-06-29T14:41:27.466960Z","shell.execute_reply.started":"2023-06-29T14:41:27.450018Z","shell.execute_reply":"2023-06-29T14:41:27.465774Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"0    5100\n9    4732\n7    3009\n3    2696\n6    2496\n2    2420\n1    2284\n8    2263\nName: score, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df_test['new_text'] = df_test['text'].map(text_preporation)\ndf_test['score'] = df_test['score'] - 1\ndf_test = df_test.drop(['text'], axis=1)\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:41:27.468799Z","iopub.execute_input":"2023-06-29T14:41:27.469158Z","iopub.status.idle":"2023-06-29T14:42:06.379703Z","shell.execute_reply.started":"2023-06-29T14:41:27.469127Z","shell.execute_reply":"2023-06-29T14:42:06.378789Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"  score                                           new_text\n0     1  another son sam definitely oscar winner techni...\n1     3  dull acting weak script worst spanish movie ye...\n2     6  i ve seen movie quite time time watch it quirk...\n3     1  interesting piece bruce weber s like dislike m...\n4     9  first saw movie mid 80 s thought funny movie g...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>score</th>\n      <th>new_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>another son sam definitely oscar winner techni...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>dull acting weak script worst spanish movie ye...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6</td>\n      <td>i ve seen movie quite time time watch it quirk...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>interesting piece bruce weber s like dislike m...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9</td>\n      <td>first saw movie mid 80 s thought funny movie g...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_test.score.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:42:06.381019Z","iopub.execute_input":"2023-06-29T14:42:06.382140Z","iopub.status.idle":"2023-06-29T14:42:06.395365Z","shell.execute_reply.started":"2023-06-29T14:42:06.382103Z","shell.execute_reply":"2023-06-29T14:42:06.394338Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0    5022\n9    4999\n7    2850\n3    2635\n2    2541\n8    2344\n6    2307\n1    2302\nName: score, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"class Vocabulary:\n    def __init__(self,freq_threshold):\n        #setting the pre-reserved tokens int to string tokens\n        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n        \n        #string to int tokens\n        #its reverse dict self.itos\n        self.stoi = {v:k for k,v in self.itos.items()}\n        \n        self.freq_threshold = freq_threshold\n        \n    def __len__(self): \n        return len(self.itos)\n    \n    @staticmethod\n    def tokenize(text):\n        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n    \n    def save_vocab(self):\n        with open('itos.txt', 'w') as f:\n            for key in self.itos.keys():\n                f.write(f'{key}:{self.itos[key]}')\n                f.write('\\n')\n                        \n        with open('stoi.txt', 'w') as f:\n            for key in self.stoi.keys():\n                f.write(f'{key}:{self.stoi[key]}')\n                f.write('\\n')\n    \n    def build_vocab(self, sentence_list):\n        frequencies = Counter()\n        idx = 4\n        \n        for sentence in sentence_list:\n            for word in self.tokenize(sentence):\n                frequencies[word] += 1\n                #add the word to the vocab if it reaches minum frequecy threshold\n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n    \n    def numericalize(self,text):\n        \"\"\" For each word in the text corresponding index token for that \n        word form the vocab built as list \"\"\"\n        tokenized_text = self.tokenize(text)\n        return [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ]","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:42:06.396839Z","iopub.execute_input":"2023-06-29T14:42:06.398111Z","iopub.status.idle":"2023-06-29T14:42:06.411237Z","shell.execute_reply.started":"2023-06-29T14:42:06.398072Z","shell.execute_reply":"2023-06-29T14:42:06.410164Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Dataset\n","metadata":{}},{"cell_type":"code","source":"class MovieDataset(Dataset):\n    \n    def __init__(self, df, valid_df=pd.DataFrame(), freq_threshold=5):\n        self.df = df\n        self.freq_threshold = freq_threshold\n        self.vocab = Vocabulary(freq_threshold)\n        self.vocab.build_vocab(self.df['new_text'].tolist())\n        if not (valid_df.empty):\n            self.df = valid_df\n        \n        \n    def __len__(self):\n        return len(self.df.new_text)\n    \n    def __getitem__(self, idx):\n        caption_vec = []\n        caption_vec += [self.vocab.stoi[\"<SOS>\"]]\n        caption_vec += self.vocab.numericalize(self.df.loc[idx, 'new_text'])\n        caption_vec += [self.vocab.stoi[\"<EOS>\"]]\n        return (torch.LongTensor(caption_vec), self.df.loc[idx, 'score'])","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:42:06.412898Z","iopub.execute_input":"2023-06-29T14:42:06.413356Z","iopub.status.idle":"2023-06-29T14:42:06.426747Z","shell.execute_reply.started":"2023-06-29T14:42:06.413326Z","shell.execute_reply":"2023-06-29T14:42:06.425652Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_dataset = MovieDataset(df)\nvalid_dataset = MovieDataset(df, df_test)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:42:06.428324Z","iopub.execute_input":"2023-06-29T14:42:06.428753Z","iopub.status.idle":"2023-06-29T14:42:31.015948Z","shell.execute_reply.started":"2023-06-29T14:42:06.428722Z","shell.execute_reply":"2023-06-29T14:42:31.014873Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    sentences = [x[0] for x in batch]\n    scores = [x[1] for x in batch]\n    sentences = pad_sequence(sentences, batch_first=True)\n    return sentences, torch.tensor(scores)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:42:31.017787Z","iopub.execute_input":"2023-06-29T14:42:31.018215Z","iopub.status.idle":"2023-06-29T14:42:31.024361Z","shell.execute_reply.started":"2023-06-29T14:42:31.018180Z","shell.execute_reply":"2023-06-29T14:42:31.023112Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\nvalid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:42:31.026070Z","iopub.execute_input":"2023-06-29T14:42:31.026476Z","iopub.status.idle":"2023-06-29T14:42:31.036349Z","shell.execute_reply.started":"2023-06-29T14:42:31.026444Z","shell.execute_reply":"2023-06-29T14:42:31.035410Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, vocab_size, embedding_size, hidden_size, n_layers=1, bidirectional=False, dropout=0.2):\n        super().__init__()\n        if bidirectional:\n            self.bi = 2\n        else:\n            self.bi = 1\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\n        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, batch_first=True, \n                           bidirectional=bidirectional)\n        self.dropout = nn.Dropout(dropout)\n        self.attention = nn.Linear(hidden_size*self.bi, 1)\n        self.fc = nn.Linear(hidden_size*self.bi, 10)\n    \n    def forward(self, x, hidden=None):\n        x = self.embedding(x)\n        out, (ht1, ct1) = self.lstm(x)\n        attention_weights = torch.softmax(self.attention(out), dim=1)\n        attended_vectors = attention_weights * out\n        context_vector = torch.sum(attended_vectors, dim=1)\n        output = self.fc(context_vector)\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:42:31.037997Z","iopub.execute_input":"2023-06-29T14:42:31.038389Z","iopub.status.idle":"2023-06-29T14:42:31.048384Z","shell.execute_reply.started":"2023-06-29T14:42:31.038358Z","shell.execute_reply":"2023-06-29T14:42:31.047340Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel = Model(len(train_dataset.vocab.itos), 256, 256, 4, True, 0.4)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:42:31.050654Z","iopub.execute_input":"2023-06-29T14:42:31.051363Z","iopub.status.idle":"2023-06-29T14:42:39.707098Z","shell.execute_reply.started":"2023-06-29T14:42:31.051331Z","shell.execute_reply":"2023-06-29T14:42:39.706162Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Model(\n  (embedding): Embedding(26001, 256)\n  (lstm): LSTM(256, 256, num_layers=4, batch_first=True, bidirectional=True)\n  (dropout): Dropout(p=0.4, inplace=False)\n  (attention): Linear(in_features=512, out_features=1, bias=True)\n  (fc): Linear(in_features=512, out_features=10, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:42:39.710178Z","iopub.execute_input":"2023-06-29T14:42:39.710463Z","iopub.status.idle":"2023-06-29T14:42:39.717751Z","shell.execute_reply.started":"2023-06-29T14:42:39.710439Z","shell.execute_reply":"2023-06-29T14:42:39.716818Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Train and test model","metadata":{}},{"cell_type":"code","source":"n_epochs = 5\nfor epoch in range(n_epochs):\n    total_loss = 0\n    train_accuracy = 0\n    valid_loss = 0\n    valid_accuracy = 0 \n    sentiment_train = 0\n    sentiment_valid = 0\n    model.train()\n    print(\"-\"*5 + f'EPOCH_{epoch}' + '-'*5)\n    for sentences, scores in tqdm(train_loader):\n        optimizer.zero_grad()\n        sentences = sentences.to(device)\n        scores = scores.to(device)\n        output = model(sentences)\n        \n        loss = criterion(output, scores)\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        \n        output = output.argmax(1).cpu().detach().numpy()\n        scores = scores.cpu().detach().numpy()\n        accuracy = accuracy_score(output, scores)\n        train_accuracy += accuracy\n        \n        output = list(map(lambda x: 0 if x<=3 else 1, output))\n        scores = list(map(lambda x: 0 if x<=3 else 1, scores))\n        accuracy = accuracy_score(output, scores)\n        sentiment_train += accuracy\n    model.eval()\n    for sentences, scores in tqdm(valid_loader):\n        sentences = sentences.to(device)\n        scores = scores.to(device)\n        output = model(sentences)\n        loss = criterion(output, scores)\n        valid_loss += loss.item()\n        \n        output = output.argmax(1).cpu().detach().numpy()\n        scores = scores.cpu().detach().numpy()\n        accuracy = accuracy_score(output, scores)\n        valid_accuracy += accuracy\n        \n        output = list(map(lambda x: 0 if x<=3 else 1, output))\n        scores = list(map(lambda x: 0 if x<=3 else 1, scores))\n        accuracy = accuracy_score(output, scores)\n        sentiment_valid += accuracy\n        \n    print(f\"Train loss: {(total_loss/len(train_loader)):.3f}\", \n          f\"Train accuracy: {(train_accuracy/len(train_loader)):.3f}\",\n          f\"Sentiment accuracy: {(sentiment_train/len(train_loader)):.3f}\")\n    print(f\"Validation loss: {(valid_loss/len(valid_loader)):.3f}\", \n          f\"Validation accuracy: {(valid_accuracy/len(valid_loader)):.3f}\",\n          f\"Sentiment accuracy: {(sentiment_valid/len(valid_loader)):.3f}\")\n        \n","metadata":{"execution":{"iopub.status.busy":"2023-06-29T14:42:39.721285Z","iopub.execute_input":"2023-06-29T14:42:39.721553Z","iopub.status.idle":"2023-06-29T15:03:41.363734Z","shell.execute_reply.started":"2023-06-29T14:42:39.721529Z","shell.execute_reply":"2023-06-29T15:03:41.362741Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"-----EPOCH_0-----\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/391 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1c67a972d494ac6b87a55dc6ab1a9fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a93b8fe12ba4038b20ecc96e8ff5ff1"}},"metadata":{}},{"name":"stdout","text":"Train loss: 1.801 Train accuracy: 0.309 Sentiment accuracy: 0.715\nValidation loss: 1.581 Validation accuracy: 0.394 Sentiment accuracy: 0.842\n-----EPOCH_1-----\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/391 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42364cf08eed478980bf4b8809f96310"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e984c43a02f40fd92bb949e2890c572"}},"metadata":{}},{"name":"stdout","text":"Train loss: 1.449 Train accuracy: 0.432 Sentiment accuracy: 0.892\nValidation loss: 1.516 Validation accuracy: 0.410 Sentiment accuracy: 0.872\n-----EPOCH_2-----\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/391 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54d2704db35b4e7cb4167c6d06ec77af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"203247fe2a1d4d5cb5548613852e4419"}},"metadata":{}},{"name":"stdout","text":"Train loss: 1.228 Train accuracy: 0.509 Sentiment accuracy: 0.937\nValidation loss: 1.474 Validation accuracy: 0.453 Sentiment accuracy: 0.874\n-----EPOCH_3-----\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/391 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e52c644ec7c6468a985603afacd92cc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0261490fd8ad451b8cb4eda61944b474"}},"metadata":{}},{"name":"stdout","text":"Train loss: 1.039 Train accuracy: 0.583 Sentiment accuracy: 0.965\nValidation loss: 1.660 Validation accuracy: 0.434 Sentiment accuracy: 0.869\n-----EPOCH_4-----\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/391 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"687270af87f24fbaaf427972fc604567"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"094dc92bd1884e49ab92994038254528"}},"metadata":{}},{"name":"stdout","text":"Train loss: 0.847 Train accuracy: 0.658 Sentiment accuracy: 0.978\nValidation loss: 1.906 Validation accuracy: 0.431 Sentiment accuracy: 0.852\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model_weights.pth')","metadata":{"execution":{"iopub.status.busy":"2023-06-29T15:03:41.365223Z","iopub.execute_input":"2023-06-29T15:03:41.366269Z","iopub.status.idle":"2023-06-29T15:03:41.461779Z","shell.execute_reply.started":"2023-06-29T15:03:41.366233Z","shell.execute_reply":"2023-06-29T15:03:41.460794Z"},"trusted":true},"execution_count":23,"outputs":[]}]}